
    <html>
      <head><meta charset="UTF-8"><title>Markdown Preview</title></head>
      <body><h2><strong>âš¡ Latency vs Throughput: The Highway Story</strong></h2>
<p>This is one of the most misunderstood concepts in system design. Let's  clear it up with a story.</p>
<h3><strong>The Road Trip Analogy</strong></h3>
<p>Imagine you need to transport 100 people from City A to City B:</p>
<p><strong>Scenario 1: The Ferrari Approach (Low Latency)</strong></p>
<p>Use a Ferrari:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</p>
<p>Speed: 200 mph (VERY FAST!)
Seats: 2 people per trip</p>
<p>Trip 1: Drive 2 people (30 minutes)</p>
<p>Trip 2: Drive 2 people (30 minutes)</p>
<p>Trip 3: Drive 2 people (30 minutes)</p>
<p>...
Trip 50: Drive last 2 people (30 minutes)</p>
<p>Total time: 25 hours</p>
<p>First person arrives: After 30 minutes âœ“ (Low latency!)
Last person arrives: After 25 hours âŒ (Low throughput!)</p>
<p><strong>Latency:</strong></p>
<p>How long ONE person takes to arrive (30 minutes)</p>
<p>âœ“ <strong>Throughput:</strong> How many people arrive per hour (4 people/hour) âŒ</p>
<p><strong>Scenario 2: The Bus Approach (High Throughput)</strong></p>
<p>Use a Bus:</p>
<p>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</p>
<p>Speed: 60 mph (slower)</p>
<p>Seats: 50 people per trip</p>
<p>Trip 1: Drive 50 people (1 hour)</p>
<p>Trip 2: Drive 50 people (1 hour)</p>
<p>Total time: 2 hours</p>
<p>First person arrives: After 1 hour âŒ (Higher latency!)</p>
<p>Last person arrives: After 2 hours âœ“ (High throughput!)</p>
<p><strong>Latency:</strong> How long ONE person takes to arrive (1 hour)</p>
<p>âŒ <strong>Throughput:</strong> How many people arrive per hour (50 people/hour) âœ“</p>
<h3><strong>The Key Insight</strong></h3>
<p><img src="https://res.cloudinary.com/dretwg3dy/image/upload/v1766473816/295_prpuky.png" alt="img1"></p>
<p><strong>Think of it this way:</strong></p>
<ul>
<li><strong>Latency:</strong> The speed of ONE trip</li>
<li><strong>Throughput:</strong> The volume over time</li>
</ul>
<h3><strong>Real System Examples</strong></h3>
<p><strong>Example 1: Web Server Comparison</strong></p>
<p>Server A: Optimized for Low Latency</p>
<p>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</p>
<p>Response time per request: 10ms (FAST!)
Concurrent requests: 100
Requests per second: 100 Ã· 0.01 = 10,000/sec</p>
<p>Great for: Real-time applications, APIs
Use case: Stock trading, gaming</p>
<p>Server B: Optimized for High Throughput</p>
<p>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</p>
<p>Response time per request: 100ms (slower)
Concurrent requests: 10,000
Requests per second: 10,000 Ã· 0.1 = 100,000/sec</p>
<p>Great for: Batch processing, data pipelines
Use case: Analytics, video encoding</p>
<p><strong>Connection to TCP:</strong> Remember TCP's flow control with sliding windows? That's managing throughput! TCP adjusts how much data flows based on network capacity. But each individual packet still has latency (round-trip time). TCP optimizes for reliable throughput, not necessarily lowest latency.</p>
<p><strong>Example 2: Database Query Design</strong></p>
<p>Let's see  a real scenario:</p>
<p>Task: Fetch 1 million user records</p>
<p>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</p>
<p>Approach A: Low Latency (One at a time)</p>
<p>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</p>
<pre><code class="language-js">for (let i = 0; i &lt; 1000000; i++) {
  const user = await db.query('SELECT * FROM users WHERE id = ?', i)
  process(user)
  // 1ms per user
}
</code></pre>
<p>Latency per user: 5ms (2ms query + 1ms process + 2ms network)
Throughput: 1,000,000 users Ã· 5ms = 200 users/second
Total time: 5,000 seconds (83 minutes!) âŒ</p>
<p>Approach B: High Throughput (Batches)</p>
<p>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</p>
<pre><code class="language-js">const BATCH_SIZE = 10000;

for (let offset = 0; offset &lt; 1000000; offset += BATCH_SIZE) {
  const users = await db.query(
    'SELECT * FROM users LIMIT ? OFFSET ?',
    BATCH_SIZE,
    offset
  )
  users.forEach(user =&gt; process(user))  // Process batch together
}
</code></pre>
<p>Latency for first batch: 100ms (slower per user!)
Throughput: 10,000 users/batch Ã— 10 batches/sec = 100,000 users/sec
Total time: 10 seconds (Much better!) âœ“</p>
<p><strong>The Tradeoff:</strong></p>
<p>Individual user latency INCREASED (5ms â†’ 100ms)
But overall throughput INCREASED 500x!</p>
<p>Sometimes you sacrifice latency for throughput!</p>
<h3><strong>The Water Pipe Analogy</strong></h3>
<p>This is a  way to explain it:</p>
<p>Latency = How fast water flows through the pipe</p>
<p>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</p>
<p><img src="https://res.cloudinary.com/dretwg3dy/image/upload/v1766473816/297_goc0k5.png" alt="img2"></p>
<p>Throughput = How much total water flows</p>
<p>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
<img src="https://res.cloudinary.com/dretwg3dy/image/upload/v1766473816/296_f2j8u7.png" alt="img3"></p>
<p>The Tradeoff:</p>
<p>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</p>
<p>Thin pipe:  Fast flow (low latency), less volume (low throughput)
Thick pipe: Slow flow (high latency), more volume (high throughput)</p>
<p>You can have:
Option A: Thin pipe with high pressure â†’ Low latency, low throughput
Option B: Thick pipe with normal pressure â†’ Higher latency, high throughput
Option C: Thick pipe with high pressure â†’ Low latency AND high throughput!
(But this is expensive! ğŸ’°)</p>
<h3><strong>Real-World Optimization Examples</strong></h3>
<p><strong>Case Study 1: Netflix Video Streaming</strong></p>
<p>Netflix's Challenge:</p>
<p>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</p>
<p>Serve 4K video to millions of users simultaneously</p>
<p>Their Solution:</p>
<p>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</p>
<p>Latency Optimization:
- CDN edge servers near users (low latency to start stream)</p>
<p>- First few seconds: High priority, low latency</p>
<p>- User sees video in &lt;1 second âœ“</p>
<p>Throughput Optimization:</p>
<p>- Pre-encode videos in multiple qualities</p>
<p>- Stream large chunks (not individual frames)</p>
<p>- Massive bandwidth pipes</p>
<p>- Serves 250 million users simultaneously âœ“</p>
<p>Result: Fast start (low latency) + sustained streaming (high throughput)</p>
<p><strong>Case Study 2: Google Search</strong></p>
<p>Google's Approach:</p>
<p>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</p>
<p>Latency Focus:</p>
<p>- Search results in &lt;100ms</p>
<p>- Users see results almost instantly</p>
<p>- Distributed data centers worldwide</p>
<p>- Aggressive caching</p>
<p>Throughput:</p>
<p>- Handles 8.5 billion searches/day</p>
<p>- 100,000 queries/second</p>
<p>- Scales horizontally</p>
<p>They optimize for BOTH:
Fast individual results + massive query volume</p>
<h3><strong>The Design Decision Framework</strong></h3>
<p>When designing systems, ask yourself:</p>
<p>Decision Tree:</p>
<p>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</p>
<p>Question 1: What matters more to your users?</p>
<p>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</p>
<p>A. &quot;I need results RIGHT NOW&quot;</p>
<p>â†’ Optimize for LOW LATENCY</p>
<p>Examples: Trading apps, games, real-time chat</p>
<p>B. &quot;I need to process LOTS of data&quot;</p>
<p>â†’ Optimize for HIGH THROUGHPUT</p>
<p>Examples: Analytics, batch jobs, video encoding</p>
<p>Question 2: What's your bottleneck?</p>
<p>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</p>
<p>A. Network speed</p>
<p>â†’ Reduce latency: CDN, edge servers, compression</p>
<p>B. Processing capacity</p>
<p>â†’ Increase throughput: More workers, parallel processing</p>
<p>C. Database</p>
<p>â†’ Balance both: Caching (latency), read replicas (throughput)</p>
<p>Question 3: What's your budget?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</p>
<p>A. Limited
â†’ Pick one: Latency OR Throughput
â†’ Can't have both cheaply</p>
<p>B. Unlimited
â†’ Get both: Fast AND scalable
â†’ Expensive but possible</p>
<h3><strong>Common Mistakes</strong></h3>
<p><strong>Mistake 1: Confusing the Two</strong></p>
<p>âŒ Wrong thinking:
&quot;My API is slow, so I'll add more servers&quot;</p>
<p>Problem: If latency is the issue, more servers might not help!</p>
<p>Example:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</p>
<p>Current: 1 server, 500ms per request
Add: 10 servers, still 500ms per request!</p>
<p>You increased throughput (10x more requests)
But latency didn't improve (still 500ms each)</p>
<p>âœ“ Correct approach for latency:</p>
<p>- Optimize the slow code</p>
<p>- Add caching</p>
<p>- Use faster database queries</p>
<p>- Reduce network hops</p>
<p><strong>Mistake 2: Optimizing the Wrong Thing</strong></p>
<p>Scenario: Batch Email System</p>
<p>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</p>
<p>Current: Sends 100,000 emails in 1 hour
Request: &quot;Make it faster!&quot;</p>
<p>Engineer A: &quot;I'll reduce latency!&quot;
Result: Each email sends in 10ms instead of 36ms
Total time: Still ~50 minutes (minor improvement)</p>
<p>Engineer B: &quot;I'll increase throughput!&quot;
Result: Process 10 emails in parallel instead of 1
Total time: 6 minutes! (10x improvement!) âœ“</p>
<p>For batch jobs: Throughput &gt; Latency</p>
</body>
    </html>