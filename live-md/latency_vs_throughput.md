## **âš¡ Latency vs Throughput: The Highway Story**

This is one of the most misunderstood concepts in system design. Let's  clear it up with a story.

### **The Road Trip Analogy**

Imagine you need to transport 100 people from City A to City B:

**Scenario 1: The Ferrari Approach (Low Latency)**

Use a Ferrari:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Speed: 200 mph (VERY FAST\!)
Seats: 2 people per trip

Trip 1: Drive 2 people (30 minutes)

Trip 2: Drive 2 people (30 minutes)

Trip 3: Drive 2 people (30 minutes)

...
Trip 50: Drive last 2 people (30 minutes)

Total time: 25 hours

First person arrives: After 30 minutes âœ“ (Low latency\!)
Last person arrives: After 25 hours âŒ (Low throughput\!)

**Latency:**

How long ONE person takes to arrive (30 minutes)

âœ“ **Throughput:** How many people arrive per hour (4 people/hour) âŒ



**Scenario 2: The Bus Approach (High Throughput)**


Use a Bus:

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Speed: 60 mph (slower)

Seats: 50 people per trip

Trip 1: Drive 50 people (1 hour)

Trip 2: Drive 50 people (1 hour)

Total time: 2 hours

First person arrives: After 1 hour âŒ (Higher latency\!)


Last person arrives: After 2 hours âœ“ (High throughput\!)

**Latency:** How long ONE person takes to arrive (1 hour)

âŒ **Throughput:** How many people arrive per hour (50 people/hour) âœ“

### **The Key Insight**


![img1](https://res.cloudinary.com/dretwg3dy/image/upload/v1766473816/295_prpuky.png)


**Think of it this way:**

* **Latency:** The speed of ONE trip
* **Throughput:** The volume over time

### **Real System Examples**

**Example 1: Web Server Comparison**

Server A: Optimized for Low Latency

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Response time per request: 10ms (FAST\!)
Concurrent requests: 100
Requests per second: 100 Ã· 0.01 \= 10,000/sec

Great for: Real-time applications, APIs
Use case: Stock trading, gaming

Server B: Optimized for High Throughput

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Response time per request: 100ms (slower)
Concurrent requests: 10,000
Requests per second: 10,000 Ã· 0.1 \= 100,000/sec

Great for: Batch processing, data pipelines
Use case: Analytics, video encoding

**Connection to TCP:** Remember TCP's flow control with sliding windows? That's managing throughput\! TCP adjusts how much data flows based on network capacity. But each individual packet still has latency (round-trip time). TCP optimizes for reliable throughput, not necessarily lowest latency.

**Example 2: Database Query Design**

Let's see  a real scenario:

Task: Fetch 1 million user records

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Approach A: Low Latency (One at a time)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```js
for (let i = 0; i < 1000000; i++) {
  const user = await db.query('SELECT * FROM users WHERE id = ?', i)
  process(user)
  // 1ms per user
}
````


Latency per user: 5ms (2ms query \+ 1ms process \+ 2ms network)
Throughput: 1,000,000 users Ã· 5ms \= 200 users/second
Total time: 5,000 seconds (83 minutes\!) âŒ

Approach B: High Throughput (Batches)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```js
const BATCH_SIZE = 10000;

for (let offset = 0; offset < 1000000; offset += BATCH_SIZE) {
  const users = await db.query(
    'SELECT * FROM users LIMIT ? OFFSET ?',
    BATCH_SIZE,
    offset
  )
  users.forEach(user => process(user))  // Process batch together
}
````



Latency for first batch: 100ms (slower per user\!)
Throughput: 10,000 users/batch Ã— 10 batches/sec \= 100,000 users/sec
Total time: 10 seconds (Much better\!) âœ“

**The Tradeoff:**

Individual user latency INCREASED (5ms â†’ 100ms)
But overall throughput INCREASED 500x\!

Sometimes you sacrifice latency for throughput\!

### **The Water Pipe Analogy**

This is a  way to explain it:

Latency \= How fast water flows through the pipe

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

![img2](https://res.cloudinary.com/dretwg3dy/image/upload/v1766473816/297_goc0k5.png)

Throughput \= How much total water flows

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
![img3](https://res.cloudinary.com/dretwg3dy/image/upload/v1766473816/296_f2j8u7.png)


The Tradeoff:

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Thin pipe:  Fast flow (low latency), less volume (low throughput)
Thick pipe: Slow flow (high latency), more volume (high throughput)

You can have:
Option A: Thin pipe with high pressure â†’ Low latency, low throughput
Option B: Thick pipe with normal pressure â†’ Higher latency, high throughput
Option C: Thick pipe with high pressure â†’ Low latency AND high throughput\!
          (But this is expensive\! ğŸ’°)

### **Real-World Optimization Examples**

**Case Study 1: Netflix Video Streaming**

Netflix's Challenge:

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Serve 4K video to millions of users simultaneously

Their Solution:

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Latency Optimization:
\- CDN edge servers near users (low latency to start stream)

\- First few seconds: High priority, low latency

\- User sees video in \<1 second âœ“

Throughput Optimization:

\- Pre-encode videos in multiple qualities

\- Stream large chunks (not individual frames)

\- Massive bandwidth pipes

\- Serves 250 million users simultaneously âœ“

Result: Fast start (low latency) \+ sustained streaming (high throughput)

**Case Study 2: Google Search**

Google's Approach:

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Latency Focus:

\- Search results in \<100ms

\- Users see results almost instantly

\- Distributed data centers worldwide

\- Aggressive caching

Throughput:

\- Handles 8.5 billion searches/day

\- 100,000 queries/second

\- Scales horizontally

They optimize for BOTH:
Fast individual results \+ massive query volume

### **The Design Decision Framework**

When designing systems, ask yourself:

Decision Tree:

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Question 1: What matters more to your users?

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


A. "I need results RIGHT NOW"

   â†’ Optimize for LOW LATENCY

   Examples: Trading apps, games, real-time chat

B. "I need to process LOTS of data"

   â†’ Optimize for HIGH THROUGHPUT

   Examples: Analytics, batch jobs, video encoding

Question 2: What's your bottleneck?

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

A. Network speed

   â†’ Reduce latency: CDN, edge servers, compression

B. Processing capacity

   â†’ Increase throughput: More workers, parallel processing

C. Database

   â†’ Balance both: Caching (latency), read replicas (throughput)

Question 3: What's your budget?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

A. Limited
   â†’ Pick one: Latency OR Throughput
   â†’ Can't have both cheaply

B. Unlimited
   â†’ Get both: Fast AND scalable
   â†’ Expensive but possible

### **Common Mistakes**

**Mistake 1: Confusing the Two**

âŒ Wrong thinking:
"My API is slow, so I'll add more servers"

Problem: If latency is the issue, more servers might not help\!

Example:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Current: 1 server, 500ms per request
Add: 10 servers, still 500ms per request\!

You increased throughput (10x more requests)
But latency didn't improve (still 500ms each)

âœ“ Correct approach for latency:

\- Optimize the slow code

\- Add caching

\- Use faster database queries

\- Reduce network hops

**Mistake 2: Optimizing the Wrong Thing**

Scenario: Batch Email System

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Current: Sends 100,000 emails in 1 hour
Request: "Make it faster\!"

Engineer A: "I'll reduce latency\!"
Result: Each email sends in 10ms instead of 36ms
Total time: Still \~50 minutes (minor improvement)

Engineer B: "I'll increase throughput\!"
Result: Process 10 emails in parallel instead of 1
Total time: 6 minutes\! (10x improvement\!) âœ“

For batch jobs: Throughput \> Latency
